{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Preparing submission file and run evaluation\n",
        "\n",
        "In this notebook, a step-by-step tutorial is provided for preparing the submission file for the shared task. To achieve this, the data for Task A, hosted on [Zenodo](https://doi.org/10.5281/zenodo.14002665), will be downloaded; a file with the appropriate [submission format](https://talentclef.github.io/talentclef/docs/talentclef-2025/evaluation/) will be prepared, and it will be evaluated using the [task's evaluation script](https://github.com/TalentCLEF/talentclef25_evaluation_script). Additionally, the provided format is also compatible with the benchmark where the test set data will be uploaded on Codabench.\n",
        "\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "TalentCLEF is an initiative to advance Natural Language Processing (NLP) in Human Capital Management (HCM). It aims to create a public benchmark for model evaluation and promote collaboration to develop fair, multilingual, and flexible systems that improve Human Resources (HR) practices across different industries.\n",
        "\n",
        "This shared-task's inaugural edition is part of the [Conference and Labs of the Evaluation Forum (CLEF)](https://clef2025.clef-initiative.eu/index.php?page=Pages/labs.html), scheduled to be held in Madrid in 2025. If you are interested in registering, you can find registration form [here](https://clef2025-labs-registration.dei.unipd.it/).\n",
        "\n",
        "<img src=\"https://github.com/TalentCLEF/talentclef/blob/main/logo_talentclef.png?raw=true\" alt=\"TalentCLEF logo\" width=\"200\"/>\n",
        "<img src=\"https://talentclef.github.io/talentclef/docs/talentclef-2025/workshop/logo_clef_madrid.png\" alt=\"TalentCLEF logo\" width=\"150\"/>\n"
      ],
      "metadata": {
        "id": "4NXz9y-YOxuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "gHTbA4g4b5Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "bLytpvDOb6ad"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Task A files"
      ],
      "metadata": {
        "id": "5u2jKkACTFM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's download the Task A and Task B zip files directly from Zenodo.\n",
        "\n"
      ],
      "metadata": {
        "id": "h6dj1eCOTIN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "!wget https://zenodo.org/records/14879510/files/TaskA.zip\n",
        "!unzip TaskA.zip -d taskA"
      ],
      "metadata": {
        "id": "At8D-dYyMJDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate releveant files using a simple model"
      ],
      "metadata": {
        "id": "FPjKPVf5XagR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load queries and corpus elements in English from the Validation folder:"
      ],
      "metadata": {
        "id": "IcVd8_ogbyeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = \"/content/taskA/validation/english/queries\"\n",
        "corpus_elements = \"/content/taskA/validation/english/corpus_elements\""
      ],
      "metadata": {
        "id": "I5n68HKmXk9M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = pd.read_csv(queries,sep=\"\\t\")\n",
        "corpus_elements = pd.read_csv(corpus_elements, sep=\"\\t\")"
      ],
      "metadata": {
        "id": "pRQGDd7CYGPu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a mapping dictionary between IDs and texts from query and corpus element strings."
      ],
      "metadata": {
        "id": "dvLGAywHcBGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries_ids = queries.q_id.to_list()\n",
        "queries_texts = queries.jobtitle.to_list()\n",
        "map_queries = dict(zip(queries_ids,queries_texts))\n",
        "\n",
        "corpus_ids = corpus_elements.c_id.to_list()\n",
        "corpus_texts = corpus_elements.jobtitle.to_list()\n",
        "map_corpus = dict(zip(queries_ids,queries_texts))"
      ],
      "metadata": {
        "id": "pRCjRJd2cGCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load simple embedding model:"
      ],
      "metadata": {
        "id": "sScTIy47cMiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OclD_jKcYSnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode queries and corpus elements:"
      ],
      "metadata": {
        "id": "rOWdNfw6cQUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = model.encode(queries_texts, convert_to_tensor=True)\n",
        "corpus_embeddings = model.encode(corpus_texts, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "PhkWx-RsYPMC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute similarities"
      ],
      "metadata": {
        "id": "8BG1vQK3cUyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = util.cos_sim(query_embeddings, corpus_embeddings).cpu().numpy()"
      ],
      "metadata": {
        "id": "yMvxZ9_dY2le"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare submission file"
      ],
      "metadata": {
        "id": "36quRAvPcZiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The submissions must follow the TREC Run File format, including headers in the output file. This means that the fle have 6 space-spearated columns per line, with following information:\n",
        "\n",
        "- q_id: Query ID.\n",
        "- Q0: A constant identifier, usually \"Q0\".\n",
        "- doc_id: ID of the retrieved document.\n",
        "- rank: Position of the document in the ranking.\n",
        "- score: Relevance score assigned by the model.\n",
        "- tag: Experiment name"
      ],
      "metadata": {
        "id": "gBZIHr6Occh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "results = []\n",
        "for q_idx, q_id in enumerate(queries_ids):\n",
        "    sorted_indices = np.argsort(-similarities[q_idx])  # Decrease order\n",
        "    for rank, c_idx in enumerate(sorted_indices[:10]):  # For this tutorial consider only 10 relevant files\n",
        "        doc_id = corpus_ids[c_idx]\n",
        "        score = similarities[q_idx, c_idx]\n",
        "        results.append(f\"{str(q_id)} Q0 {str(doc_id)} {rank+1} {score:.4f} baseline_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eNZ_KNAbZDWw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list has this structure"
      ],
      "metadata": {
        "id": "27m9mMwYds4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB4KtEPldtE3",
        "outputId": "c61357b7-a103-4bd6-c87a-87875d875e81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 Q0 870 1 0.6761 baseline_model', '1 Q0 2016 2 0.6558 baseline_model']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the list as a file:"
      ],
      "metadata": {
        "id": "7AGaIVV9dxYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"evaluation_baseline.trec\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(results))"
      ],
      "metadata": {
        "id": "eRnSEkckdrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "DZBOb2Iucq5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the evaluation, we will use the official [TalentCLEF evaluation script](https://github.com/TalentCLEF/talentclef25_evaluation_script), which uses the Ranx library under the hood.\n",
        "\n",
        "First, clone the repo and install the requirements file:"
      ],
      "metadata": {
        "id": "A8l5IZancsMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TalentCLEF/talentclef25_evaluation_script.git\n",
        "!pip install -r /content/talentclef25_evaluation_script/requirements.txt\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jKr2js7xZVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, select the Qrels file and the Run file to perform the evaluation.\n"
      ],
      "metadata": {
        "id": "iS5VMHMUdBdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels_file = \"/content/taskA/validation/english/qrels.tsv\"\n",
        "run_file = \"/content/evaluation_baseline.trec\""
      ],
      "metadata": {
        "id": "3ny9Xm2BZu0f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = [\"python\", \"/content/talentclef25_evaluation_script/talentclef_evaluate.py\", \"--qrels\", qrels_file, \"--run\", run_file]\n",
        "result = subprocess.run(command, capture_output=True, text=True)\n",
        "print(result.stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHlKBSMxZ_bB",
        "outputId": "f4e5d446-94cb-44d3-9fc8-f1c66f15990a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received parameters:\n",
            "  qrels: /content/taskA/validation/english/qrels.tsv\n",
            "  run: /content/evaluation_baseline.trec\n",
            "Loading qrels...\n",
            "Loading run...\n",
            "Running evaluation...\n",
            "\n",
            "=== Evaluation Results ===\n",
            "map: 0.2923\n",
            "mrr: 0.7609\n",
            "ndcg: 0.4296\n",
            "precision@5: 0.6762\n",
            "precision@10: 0.5914\n",
            "precision@100: 0.0591\n",
            "\n"
          ]
        }
      ]
    }
  ]
}