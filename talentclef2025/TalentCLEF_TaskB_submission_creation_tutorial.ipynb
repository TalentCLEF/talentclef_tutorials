{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Preparing submission file and run evaluation for Task B\n",
    "\n",
    "In this notebook, a step-by-step tutorial is provided for preparing the submission file for the shared task Task B. To achieve this, the data for Task B, hosted on [Zenodo](https://doi.org/10.5281/zenodo.14002665), will be downloaded; a file with the appropriate [submission format](https://talentclef.github.io/talentclef/docs/talentclef-2025/evaluation/) will be prepared, and it will be evaluated using the [task's evaluation script](https://github.com/TalentCLEF/talentclef25_evaluation_script). Additionally, the provided format is also compatible with the benchmark where the test set data will be uploaded on Codabench.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "TalentCLEF is an initiative to advance Natural Language Processing (NLP) in Human Capital Management (HCM). It aims to create a public benchmark for model evaluation and promote collaboration to develop fair, multilingual, and flexible systems that improve Human Resources (HR) practices across different industries.\n",
    "\n",
    "This shared-task's inaugural edition is part of the [Conference and Labs of the Evaluation Forum (CLEF)](https://clef2025.clef-initiative.eu/index.php?page=Pages/labs.html), scheduled to be held in Madrid in 2025. If you are interested in registering, you can find registration form [here](https://clef2025-labs-registration.dei.unipd.it/).\n",
    "\n",
    "<img src=\"https://github.com/TalentCLEF/talentclef/blob/main/logo_talentclef.png?raw=true\" alt=\"TalentCLEF logo\" width=\"200\"/>\n",
    "<img src=\"https://talentclef.github.io/talentclef/docs/talentclef-2025/workshop/logo_clef_madrid.png\" alt=\"TalentCLEF logo\" width=\"150\"/>\n"
   ],
   "metadata": {
    "id": "4NXz9y-YOxuY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "id": "gHTbA4g4b5Na"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import subprocess\n",
    "from codecarbon import EmissionsTracker"
   ],
   "metadata": {
    "id": "bLytpvDOb6ad"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Task B files"
   ],
   "metadata": {
    "id": "5u2jKkACTFM_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's download the Task A and Task B zip files directly from Zenodo.\n",
    "\n"
   ],
   "metadata": {
    "id": "h6dj1eCOTIN8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download\n",
    "!wget https://zenodo.org/records/15038364/files/TaskB.zip\n",
    "!unzip TaskB.zip -d taskB"
   ],
   "metadata": {
    "id": "At8D-dYyMJDt",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate releveant files using a simple model"
   ],
   "metadata": {
    "id": "FPjKPVf5XagR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DE MOMENTO SUBIR LOS DATOS A MANOS HASTA EL RELEASE.**"
   ],
   "metadata": {
    "id": "SpXxZkyeEbHm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load queries and corpus elements in English from the Validation folder:"
   ],
   "metadata": {
    "id": "IcVd8_ogbyeH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "queries = \"/content/taskB/validation/queries\"\n",
    "corpus_elements = \"/content/taskB/validation/corpus_elements\""
   ],
   "metadata": {
    "id": "I5n68HKmXk9M"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "queries = pd.read_csv(queries,sep=\"\\t\")\n",
    "corpus_elements = pd.read_csv(corpus_elements, sep=\"\\t\")\n"
   ],
   "metadata": {
    "id": "pRQGDd7CYGPu"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform `skill_aliases` column to a list of strings:"
   ],
   "metadata": {
    "id": "1V2kAegcrZM3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "corpus_elements[\"skill_aliases\"] = corpus_elements[\"skill_aliases\"].apply(lambda x: ast.literal_eval(x))"
   ],
   "metadata": {
    "id": "8Yr1jIxtoj8V"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a mapping dictionary between IDs and texts from query and corpus element strings."
   ],
   "metadata": {
    "id": "dvLGAywHcBGl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "queries_ids = queries.q_id.to_list()\n",
    "queries_texts = queries.jobtitle.to_list()\n",
    "map_queries = dict(zip(queries_ids,queries_texts))"
   ],
   "metadata": {
    "id": "MAoY01hXoSUz"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before creating a mapping dictionary of texts to corpus_ids, explode the `skill_aliases` column:"
   ],
   "metadata": {
    "id": "3ke4L-amrgqF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "list_aliases_df = corpus_elements.explode(\"skill_aliases\")"
   ],
   "metadata": {
    "id": "egGCX2E_x81O"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "corpus_ids = list_aliases_df.c_id.to_list()\n",
    "corpus_texts = list_aliases_df.skill_aliases.to_list()\n",
    "map_corpus = dict(zip(corpus_texts,corpus_ids))"
   ],
   "metadata": {
    "id": "pRCjRJd2cGCx"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load simple embedding model:"
   ],
   "metadata": {
    "id": "sScTIy47cMiC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "OclD_jKcYSnN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tracker = EmissionsTracker()\n",
    "tracker.start_task(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encode queries and corpus elements:"
   ],
   "metadata": {
    "id": "rOWdNfw6cQUi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query_embeddings = model.encode(queries_texts, convert_to_tensor=True)\n",
    "corpus_embeddings = model.encode(corpus_texts, convert_to_tensor=True)"
   ],
   "metadata": {
    "id": "PhkWx-RsYPMC"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute similarities"
   ],
   "metadata": {
    "id": "8BG1vQK3cUyt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "similarities = util.cos_sim(query_embeddings, corpus_embeddings).cpu().numpy()\n",
    "emissions = tracker.stop_task(\"all-MiniLM-L6-v2\")"
   ],
   "metadata": {
    "id": "yMvxZ9_dY2le"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare submission file"
   ],
   "metadata": {
    "id": "36quRAvPcZiA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The submissions must follow the TREC Run File format, including headers in the output file. This means that the fle have 6 space-spearated columns per line, with following information:\n",
    "\n",
    "- q_id: Query ID.\n",
    "- Q0: A constant identifier, usually \"Q0\".\n",
    "- doc_id: ID of the retrieved document.\n",
    "- rank: Position of the document in the ranking.\n",
    "- score: Relevance score assigned by the model.\n",
    "- tag: Experiment name"
   ],
   "metadata": {
    "id": "gBZIHr6Occh1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "results = []\n",
    "results_name = []\n",
    "\n",
    "for q_idx, q_id in enumerate(queries_ids):\n",
    "    sorted_indices = np.argsort(-similarities[q_idx])\n",
    "    used_doc_ids = set()\n",
    "    rank_counter = 0\n",
    "    for c_idx in sorted_indices:  # Consider the full list.\n",
    "        doc_id = corpus_ids[c_idx]\n",
    "        # If doc_id was already processed, go to the next one.\n",
    "        if doc_id in used_doc_ids:\n",
    "            continue\n",
    "        used_doc_ids.add(doc_id)\n",
    "        rank_counter += 1\n",
    "\n",
    "        query_name = map_queries[q_id]\n",
    "        doc_name = corpus_texts[c_idx]\n",
    "        score = similarities[q_idx, c_idx]\n",
    "\n",
    "        results.append(f\"{q_id} Q0 {doc_id} {rank_counter} {score:.4f} baseline_model\")\n",
    "        results_name.append(f\"{query_name} Q0 {doc_name} {rank_counter} {score:.4f} baseline_model\")"
   ],
   "metadata": {
    "id": "eNZ_KNAbZDWw"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The list has this structure"
   ],
   "metadata": {
    "id": "27m9mMwYds4o"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's save the list as a file:"
   ],
   "metadata": {
    "id": "7AGaIVV9dxYT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"evaluation_baseline_taskB.trec\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(results))\n",
    "\n",
    "json.dump(dict(emissions.values), open(\"./emissions.json\", \"w\"), ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "id": "eRnSEkckdrWA"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "id": "DZBOb2Iucq5Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the evaluation, we will use the official [TalentCLEF evaluation script](https://github.com/TalentCLEF/talentclef25_evaluation_script), which uses the Ranx library under the hood.\n",
    "\n",
    "First, clone the repo and install the requirements file:"
   ],
   "metadata": {
    "id": "A8l5IZancsMu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/TalentCLEF/talentclef25_evaluation_script.git\n",
    "!pip install -r /content/talentclef25_evaluation_script/requirements.txt\n"
   ],
   "metadata": {
    "collapsed": true,
    "id": "jKr2js7xZVEl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, select the Qrels file and the Run file to perform the evaluation.\n"
   ],
   "metadata": {
    "id": "iS5VMHMUdBdh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "qrels_file = \"/content/taskB/validation/qrels.tsv\"\n",
    "run_file = \"/content/evaluation_baseline_taskB.trec\""
   ],
   "metadata": {
    "id": "3ny9Xm2BZu0f"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "command = [\"python\", \"/content/talentclef25_evaluation_script/talentclef_evaluate.py\", \"--qrels\", qrels_file, \"--run\", run_file]\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "print(result.stdout)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHlKBSMxZ_bB",
    "outputId": "062f7579-4bcc-4d9d-9929-088f657002ba"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Received parameters:\n",
      "  qrels: /content/taskB/validation/qrels.tsv\n",
      "  run: /content/evaluation_baseline_taskB.trec\n",
      "Loading qrels...\n",
      "Loading run...\n",
      "Running evaluation...\n",
      "\n",
      "=== Evaluation Results ===\n",
      "map: 0.1874\n",
      "mrr: 0.6752\n",
      "ndcg: 0.6660\n",
      "precision@5: 0.4500\n",
      "precision@10: 0.3852\n",
      "precision@100: 0.1964\n",
      "\n"
     ]
    }
   ]
  }
 ]
}
